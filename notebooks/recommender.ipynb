{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "interpreter: python\n",
    "\n",
    "# Embedding-based Recommender\n",
    "\n",
    "## Problem\n",
    "\n",
    "Maximize P(item|observation), i.e. the probability that given an observation (like\n",
    "someone with a certain profile feature --age, gender-- or a _past_ action --a purchase, \n",
    "a click on an item) we can find the items at higher probability of being actioned.\n",
    "\n",
    "This means we put in the NN the one-hot of features and make the cross-entropy\n",
    "of the resulting one-hot with item.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0K1ZyLn04QZf"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "# import zipfile\n",
    "#from matplotlib import pylab\n",
    "#from six.moves import range\n",
    "#from six.moves.urllib.request import urlretrieve\n",
    "#from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCjPJE944bkV"
   },
   "source": [
    "### Read the files (and sort the actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28844,
     "status": "ok",
     "timestamp": 1445964497165,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d"
   },
   "outputs": [],
   "source": [
    "# ID devono essere dal piu` frequente al meno frequente (cfr https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)\n",
    "def read_data(user_file, item_file, action_file,\n",
    "              max_actions=0, actions_cutoff=100, sort_actions=False, \n",
    "              separator='|', header=False):\n",
    "    '''Read the files and builds the dicts.\n",
    "    \n",
    "        :param str user_file: id, feature1, feature2 etc\n",
    "        :param str item_file: id, category, description(=words comma-separated)\n",
    "        :param str action_file: user_id, item_id, timestamp (NB all actions by the same users TOGETHER)\n",
    "        :param int max_actions: max number of actions to be read\n",
    "        :param int actions_cutoff: max actions to be stored _per user_ (some Anobii users are actually bookshop with 1000s of books)\n",
    "        :param bool sort_actions: set to True if actions are not timestamp sorted\n",
    "        :param str separator: for the csv (\"|\")\n",
    "        :return Tuple(Dict, Dict, Dict): users = Uid -> [P1, ..., Pn], \n",
    "                                         items = Iid -> [I_0, ..., I_m],\n",
    "                                         obs = Uid -> [(Iid, Timestamp, Action), (), ..., ()]\n",
    "    \n",
    "    '''\n",
    "    # dicts\n",
    "    users = {}  # Uid -> [P1, ..., Pn]\n",
    "    items = {}  # Iid -> [I_0, ..., I_m]\n",
    "    obs = {}  # Uid -> [(Timestamp, Iid), (), ..., ()]\n",
    "    print(\"INFO: Reading user file \" + user_file)            \n",
    "    with open(user_file) as f:\n",
    "        if header:\n",
    "            f.readline()  # header\n",
    "        for line in f.readlines():\n",
    "            v = line.strip(\"\\n\").split(separator)\n",
    "            try:\n",
    "                if len(v) > 1:\n",
    "                    users[v[0]] = v[1:]\n",
    "                else:  # for the moment we only have the id\n",
    "                    users[v[0]] = []\n",
    "            except:\n",
    "                continue\n",
    "    print(\"INFO: Done with users\")\n",
    "    \n",
    "    print(\"INFO: Reading item file \" + item_file)\n",
    "    # item[book_id] = [title in word_id, author_id, lang_id, time]\n",
    "    with open(item_file) as f:\n",
    "        if header:\n",
    "            f.readline()\n",
    "        for line in f.readlines():\n",
    "            v = line.strip(\"\\n\").split(separator)\n",
    "            items[v[0]] = v[1:]\n",
    "    print(\"INFO: Done with items\")\n",
    "\n",
    "    # how much is onepc (1%)?\n",
    "    if max_actions==0:\n",
    "        with open(action_file) as f:\n",
    "            onepc = int(sum(1 for line in f)/10)\n",
    "    else:\n",
    "        onepc = int(max_actions/10)\n",
    "\n",
    "    idx = 0\n",
    "    print(\"INFO: Reading action file \" + action_file)    \n",
    "    with open(action_file) as f:\n",
    "        if header:\n",
    "            titles = dict([(n,i) for i,n in enumerate(f.readline().strip(\"\\n\").split(separator))])\n",
    "        u = ''\n",
    "        for line in f.readlines():            \n",
    "            idx += 1\n",
    "            if idx%onepc==0:\n",
    "                pc = idx//onepc\n",
    "                print(\"%s0 percent actions analyzed (%s)\" % (pc, idx))\n",
    "            v = line.strip(\"\\n\").split(separator)            \n",
    "            if u != '' and u != v[0]:  # it's a new user, then...\n",
    "                # sort previous array of observations if it's not a \n",
    "                # user with huge number of transactions (i.e. useless, like\n",
    "                # librarians, bookshop etc)\n",
    "                o = obs.get(u)\n",
    "                if o is not None:  # it happens for users who have obs with too rare items\n",
    "                    if len(o)<actions_cutoff or actions_cutoff==0:\n",
    "                        if sort_actions:\n",
    "                            obs[u] = sorted(o)\n",
    "                        else:\n",
    "                            obs[u] = o\n",
    "                    else:\n",
    "                        del(obs[u])\n",
    "            u = str(v[0])\n",
    "            if header:\n",
    "                obs.setdefault(str(v[titles['user']]), []).append((int(v[titles['timestamp']]), str(v[titles['item']])))\n",
    "            else:\n",
    "                obs.setdefault(str(v[0]), []).append((int(v[2]), str(v[1])))\n",
    "            if max_actions != 0 and idx >= max_actions:\n",
    "                break\n",
    "                \n",
    "    return users, items, obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files for obs, items and users if PICKE are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_obs = pickle.load(open(\"anobii_actions.pickle\", \"rb\"))\n",
    "# items = pickle.load(open(\"anobii_items.pickle\", \"rb\"))\n",
    "# users = pickle.load(open(\"anobii_users.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otherwise produce the pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use real or synthetic data\n",
    "synthetic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if synthetic:\n",
    "    sort_actions = False\n",
    "    user_file = 'synthetic/us.csv'\n",
    "    item_file = 'synthetic/is.csv'\n",
    "    action_file = 'synthetic/os.csv'\n",
    "    actions_cutoff = 0\n",
    "    max_actions = 0\n",
    "    separator = \"\\t\"\n",
    "    header = True\n",
    "else:\n",
    "    sort_actions = False\n",
    "    user_file = 'anobii_users.csv'\n",
    "    item_file = 'anobii_items.csv'\n",
    "    action_file = 'anobii_actions.csv'\n",
    "    actions_cutoff = 500\n",
    "    max_actions = 0  \n",
    "    separator = '|'\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, obs = read_data(user_file=user_file, item_file=item_file, \n",
    "                              action_file=action_file, max_actions=max_actions,\n",
    "                             actions_cutoff=actions_cutoff, header=header, separator=separator)\n",
    "print('# users %d' % len(users))\n",
    "print('# items %d' % len(items))\n",
    "print('# obs %d' % len(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N users: \", len(users))\n",
    "print(\"max user_id: \", max([int(i) for i in users.keys()]))\n",
    "print(\"N items: \", len(items))\n",
    "print(\"max item_id: \", max([int(i) for i in items.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary book_id => title (to be used for retriving titles in recommendation's test)\n",
    "if not synthetic:\n",
    "    bookid_title = {}\n",
    "    title_bookid = {}\n",
    "    with open(\"title_bookid.csv\") as f:    \n",
    "        for l in f.readlines():\n",
    "            v, k = l.strip(\"\\n\").split(\"|\")\n",
    "            bookid_title[int(k)] = v\n",
    "            title_bookid[v] = int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookid_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs['228054']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean obs: take out users who bought very little\n",
    "clean_obs = {}\n",
    "idx = 0\n",
    "for u in obs.keys():\n",
    "    idx += 1\n",
    "    if idx%50000==0:\n",
    "        print(\"done \", idx)\n",
    "    if len(obs[u]) > 2:\n",
    "        clean_obs[u] = obs[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 104415\n",
    "user = '10476' #'104415'  # marioalemi\n",
    "print(\"User\", user, \"has\", len(clean_obs[user]), \"obs.\\nFirst obs:\\n\", clean_obs[user][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[bookid_title[int(o[1])] for o in clean_obs['104415']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"observations.pickle\", 'wb') as f:\n",
    "#     pickle.dump(clean_obs, f)\n",
    "# with open(\"items.pickle\", 'wb') as f:\n",
    "#     pickle.dump(items, f)\n",
    "# with open(\"users.pickle\", 'wb') as f:\n",
    "#     pickle.dump(users, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare list of observations with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fa una lista\n",
    "def build_obs_dataset_list(observations, roll_over=10, \n",
    "                           decay_type=None, decay_period=86400*365, \n",
    "                           max_item=5, n_users=0):\n",
    "    '''\n",
    "\n",
    "    :par observations: '73462': [('1379808000', '30509'),  ('1382054400', '21328'),  ('1382054400', '46409', '3')]\n",
    "    :par roll_over: takes as last trainer the past roll_over items (eventually weighted with the previous max_items items preceeding the last_trainer)\n",
    "    :par str decay_type: in ['exponential', 'inverse_sqrt', 'inverse']. inverse sqrt should be the best...\n",
    "    :par str decay_period: unit of time in sec for the decay (eg 86400*7 for weeks)\n",
    "    :par int max_items: number of previous items to be considered\n",
    "    :par int n_obs: number of obs to analyze\n",
    "    :return\n",
    "    [(UID1, LABEL_ITEM_ID, [(TRAINER_ITEM_ID, WEIGHT), (TRAINER_ITEM_ID, WEIGHT), ...]),  \n",
    "     (UID1, LABEL_ITEM_ID, [(TRAINER_ITEM_ID, WEIGHT), (TRAINER_ITEM_ID, WEIGHT), ....]), \n",
    "     ...\n",
    "     (UID2, LABEL_ITEM_ID, [(TRAINER_ITEM_ID, WEIGHT), (TRAINER_ITEM_ID, WEIGHT), ...]), \n",
    "     ....\n",
    "    ]      \n",
    "    \n",
    "    '''\n",
    "    if decay_type == 'exponential':\n",
    "        def fdecay(action_time, label_time):\n",
    "            return math.exp((-action_time+label_time)/decay_period)\n",
    "    elif decay_type == 'inverse_sqrt':\n",
    "        def fdecay(action_time, label_time):\n",
    "#            print(\"action_time, label_time decay\", action_time, label_time, decay_period)\n",
    "            if action_time == label_time:\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 1.0/math.sqrt((-action_time+label_time)/decay_period)\n",
    "    elif decay_type == 'inverse':\n",
    "        def fdecay(action_time, label_time):\n",
    "            if action_time == label_time:\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 1.0/((-action_time+label_time)/decay_period)\n",
    "    elif decay_type == None:\n",
    "        def fdecay(action_time, label_time):\n",
    "            return 1.0\n",
    "    else:\n",
    "        raise ValueError(\"decay_type must be 'exponential', 'inverse_sqrt', 'inverse' or None\")\n",
    "    \n",
    "    result = []\n",
    "    if n_users == 0:\n",
    "        n_users = len(observations)\n",
    "    onepc = int(n_users/10)        \n",
    "    for u in list(observations.keys())[:n_users]:\n",
    "        # print(\"\\n\\n================================\\nuser \", u)\n",
    "        pairs = []\n",
    "        for index, action in enumerate(observations[u]):  \n",
    "            if index == 0:\n",
    "                continue  # ([1:] doesn't work) bc first item has no trainers\n",
    "            # print(\"\\tindex action\", index, action)\n",
    "            label_time = int(action[0])\n",
    "            label = int(action[1])            \n",
    "            for last_trainer in range(max(0, index - roll_over), index):\n",
    "                trainers = []\n",
    "                # print(\"\\t\\tlast_trainer\", observations[u][last_trainer])\n",
    "                for trainer_time, trainer in observations[u][max(0, last_trainer-max_item+1):last_trainer+1]:\n",
    "                    # print(\"\\t\\t\\ttrainer_time, trainer\", trainer_time, trainer)\n",
    "                    weight = fdecay(int(trainer_time), label_time)\n",
    "                    trainers.append((int(trainer), weight))\n",
    "                result.append((int(u), label, trainers))\n",
    "    random.shuffle(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load obs_dataset_list if pickle is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_dataset_list = pickle.load(open(\"obs_dataset_list.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the obs_dataset_list if pickle NOT THERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if synthetic:  # let's start with just one item as input and the next bought as label\n",
    "    roll_over=1,\n",
    "    decay_type=None, #'inverse_sqrt', \n",
    "    decay_period=86400*3650, \n",
    "    max_item=1,\n",
    "    n_users=0  # analyze all users\n",
    "else:\n",
    "    roll_over=10,\n",
    "    decay_type=None, #'inverse_sqrt', \n",
    "    decay_period=86400*3650, \n",
    "    max_item=1,\n",
    "    n_users=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dataset_list = build_obs_dataset_list(observations=clean_obs, \n",
    "                                          roll_over=10,\n",
    "                                          decay_type=None, #'inverse_sqrt', \n",
    "                                          decay_period=86400*3650, \n",
    "                                          max_item=1,\n",
    "                                          n_users=n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dataset_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"obs_dataset_list.pickle\", 'wb') as f:\n",
    "#     pickle.dump(obs_dataset_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(obs_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''[(242438400, '15494'), \n",
    " (271814400, '123'), \n",
    " (468806400, '112043'), \n",
    " (1370390400, '279'), \n",
    " (1669766400, '35')]\n",
    "'''\n",
    "[o for o in obs_dataset_list if str(o[0]) == '168088']\n",
    "#[o for o in obs_dataset_list if str(o[0]) == '11339']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DON'T CONSIDER ###\n",
    "# Create a \"test_obs_dataset_list\" where for each obs we insert a similar book to the label\n",
    "\n",
    "super_fake = False\n",
    "\n",
    "# Check for similar books (books with the same label)\n",
    "title = 'gabriel garcia marquez_cronaca di una morte annunciata'\n",
    "#title = \"gabriel garcia marquez_cent'anni di solitudine\"\n",
    "bid = title_bookid[title]\n",
    "similar_title = \"gabriel garcia marquez_cent'anni di solitudine\"\n",
    "sid = title_bookid[similar_title]\n",
    "counter = {}\n",
    "in_training = 0\n",
    "test_obs_dataset_list = []\n",
    "negative_samples = 0\n",
    "\n",
    "for o in obs_dataset_list:\n",
    "    batch = [bw[0] for bw in o[2]]\n",
    "    if bid in batch:\n",
    "        in_training += 1\n",
    "        #print(o)\n",
    "        fake_o = []\n",
    "        for b in batch:\n",
    "            if b != bid:\n",
    "                if b != sid:\n",
    "                    fake_o.append((b, 1.0))\n",
    "                counter[bookid_title[b]] = counter.get(bookid_title[b], 0) + 1\n",
    "        if super_fake:\n",
    "            try:\n",
    "                test_obs_dataset_list.append((o[0], o[1], \n",
    "                                          [(bid, 1.0), (sid, 1.0), fake_o[0], fake_o[1]]))\n",
    "            except:\n",
    "                pass #sometimes there are less than 4 trainers.....\n",
    "        else:\n",
    "            test_obs_dataset_list.append(o)\n",
    "    else:\n",
    "        if negative_samples > 0:\n",
    "            test_obs_dataset_list.append(o)\n",
    "            negative_samples -= 1\n",
    "\n",
    "print(title, \" trainato \", in_training , \" volte.\")\n",
    "print(\"ID: \", bid)\n",
    "print(sorted(counter.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs_dataset_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch_constant(obs_dataset_list, item_titles, data_index, \n",
    "                            batch_size, max_words, max_items):\n",
    "    '''\n",
    "    Constant batch size\n",
    "\n",
    "    A dictionary with all matrices and indexes. \n",
    "    \n",
    "    IN\n",
    "    obs_dataset: [[user, label, item1, item2], [...], ...]\n",
    "    item_titles: titles of books with word_ids\n",
    "    max_words: max number of words used in the title\n",
    "    max_items: max number of past actioned items used for the training\n",
    "    batch_size: number of users to be analyzed in the same batch. #TODO: obs_dataset could repeat the user for each sublist.\n",
    "    \n",
    "    OUT\n",
    "    \n",
    "    A 2-tuple:\n",
    "    \n",
    "    [0] type(dict):\n",
    "        \"one_hot_words_values\": [1st word in 1st batch, 2nd in 1st, ..., max_words'th in batch_size'th]\n",
    "        \"one_hot_words_weights\": [weight of 1st word in 1st batch, 2nd in 1st, ..., max_words'th in batch_size'th]\n",
    "        \"words_indices\": [[0,0], [0, 1] .. [0, batch_size], [1, 0] ... [max_words, batch_size]]\n",
    "        \"one_hot_items_values\": [1st item in 1st batch, 2nd in 1st, ..., max_words'th in batch_size'th]\n",
    "        \"one_hot_items_weights\": as for words but with \"items\"\n",
    "        \"items_indices\": as for words but with \"items\"\n",
    "        # \"profiles\": a 3d vector with each element [-1, +1] made of [gender, age, job]\n",
    "        \"labels\": the items bought by the user\n",
    "\n",
    "     [1]: type(int): new data_index \n",
    "    '''\n",
    "    \n",
    "    one_hot_words_values = []\n",
    "    one_hot_words_weights = []\n",
    "        \n",
    "    one_hot_items_values = []\n",
    "    one_hot_items_weights = []\n",
    "    \n",
    "#    profiles = []\n",
    "    labels = []\n",
    "    total_obs = 0\n",
    "    batch_idx = 0\n",
    "    idx = (data_index + batch_idx) % len(obs_dataset_list)\n",
    "    n_loop = 0\n",
    "    while(batch_idx < batch_size):\n",
    "        n_loop += 1\n",
    "        if n_loop > 100 and batch_idx == 0:\n",
    "            msg = \"generate_batch_constant has empty loops \" + str(n_loop) + \", batch_idx: \" + str(batch_idx)\n",
    "            raise Exception(msg)\n",
    "        usr_lab_obs = obs_dataset_list[idx]\n",
    "        user = usr_lab_obs[0]\n",
    "#        print(\"USER: \", user)\n",
    "        label = usr_lab_obs[1]\n",
    "        trainers = usr_lab_obs[2]\n",
    "        \n",
    "        # get one-hot value of all words in the training items\n",
    "#        if max_words > 0:\n",
    "        word_value = [int(word) for item_weight in trainers\n",
    "                       for word in item_titles.get(str(item_weight[0]), [])][:max_words]\n",
    "        word_weight = [float(item_weight[1]) for item_weight in trainers\n",
    "                       for word in item_titles.get(str(item_weight[0]), [])][:max_words]\n",
    "\n",
    "        word_value.extend([0] * (max_words-len(word_value)))\n",
    "#        print('word_value ->', word_value)\n",
    "        word_weight.extend([0] * (max_words-len(word_weight)))\n",
    "#        print('word_weight ->', word_weight)\n",
    "\n",
    "        item_value = [item_weight[0] for item_weight in trainers][:max_items]\n",
    "        item_weight = [item_weight[1] for item_weight in trainers][:max_items]\n",
    "        item_value.extend([0] * (max_items-len(item_value)))            \n",
    "#        print('item_value ->', item_value)\n",
    "        item_weight.extend([0] * (max_items-len(item_weight)))\n",
    "#        print('item_weight ->', item_weight)\n",
    "\n",
    "        usr_str = str(user)\n",
    "    #    profile = users[usr_str]\n",
    "        # item \"0\" is not valid. #TODO a better method...\n",
    "        if not synthetic and (sum(item_value)==0 or sum(item_weight)==0 or ((sum(word_value)==0 or sum(word_weight)==0) and max_words > 0)):\n",
    "            # if there are no words but max_words>0, go on reading w/o add to batch_idx\n",
    "            idx = (idx + 1) % len(obs_dataset_list)  \n",
    "            #print(\"Something wrong with user %s at index %s and batch_idx %s\" % (user, idx, batch_idx))\n",
    "        else:\n",
    "#             if max_words > 0:\n",
    "#                 one_hot_words_values += word_value\n",
    "#                 one_hot_words_weights += word_weight\n",
    "            one_hot_items_values += item_value\n",
    "            one_hot_items_weights += item_weight        \n",
    "#            profiles.append(profile)\n",
    "            labels.append([label])  # each label is in reality a one-hot representation to be embedded\n",
    "            batch_idx += 1\n",
    "            idx = (data_index + batch_idx) % len(obs_dataset_list)\n",
    "#            print(\">>>>> GOOD \", user, obs, idx, batch_idx)            \n",
    "\n",
    "\n",
    "#     print('one_hot_words_values  ', one_hot_words_values)\n",
    "#     print('one_hot_words_weights  ', one_hot_words_weights)\n",
    "#     print('LENGTH one_hot_words_values  ', len(one_hot_words_values))\n",
    "#     print('LENGTH one_hot_words_weights  ', len(one_hot_words_weights))\n",
    "#     print('one_hot_items_values  ', one_hot_items_values)\n",
    "#     print('LENGTH one_hot_items_values  ',len(one_hot_items_values))\n",
    "#     print('profiles ', profiles)\n",
    "    \n",
    "            \n",
    "    if max_words > 0:\n",
    "        return  {'one_hot_words_values': np.array(one_hot_words_values, dtype=np.float32), \n",
    "                 'one_hot_words_weights': np.array(one_hot_words_weights, dtype=np.float32),\n",
    "                 'one_hot_items_values': np.array(one_hot_items_values, dtype=np.float32),\n",
    "                 'one_hot_items_weights': np.array(one_hot_items_weights, dtype=np.float32),\n",
    "    #             'profiles': np.array(profiles, dtype=np.float32),\n",
    "                 'labels': np.array(labels, dtype=np.float32)\n",
    "                }, (data_index + batch_size) % len(obs_dataset_list)\n",
    "    else:\n",
    "        return  {'one_hot_items_values': np.array(one_hot_items_values, dtype=np.float32),\n",
    "                 'one_hot_items_weights': np.array(one_hot_items_weights, dtype=np.float32),\n",
    "    #             'profiles': np.array(profiles, dtype=np.float32),\n",
    "                 'labels': np.array(labels, dtype=np.float32)\n",
    "                }, (data_index + batch_size) % len(obs_dataset_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if synthetic:\n",
    "    item_titles = dict((i, []) for i in items)\n",
    "else:\n",
    "    item_titles = dict((k, [w.strip() for w in v[0].split(',')  if len(w.strip()) > 0]) for k,v in items.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if synthetic:\n",
    "    assert(item_titles['2'] == [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = obs_dataset_list\n",
    "#test:\n",
    "#dataset = test_obs_dataset_list\n",
    "\n",
    "max_words = 0\n",
    "max_items = 1\n",
    "batch_size = 10\n",
    "test_new_data_index = 40\n",
    "tbatch, test_new_data_index = generate_batch_constant(obs_dataset_list=dataset, \n",
    "                                                  item_titles=item_titles,\n",
    "                                                  data_index=test_new_data_index, \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  max_words=max_words, max_items=max_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Len of the dataset: \", len(dataset))\n",
    "print(\"New data index: \", test_new_data_index)\n",
    "print(\"max word * batch size: \", max_words*batch_size)\n",
    "try:\n",
    "    print(\"Same number of events for words and items: \", len(tbatch[\"one_hot_words_values\"])/max_words == len(tbatch[\"one_hot_items_values\"])/max_items)\n",
    "    print(\"Batches of words: \", (tbatch[\"one_hot_words_values\"]))\n",
    "    print(\"Batches of words' weights: \", (tbatch[\"one_hot_words_weights\"]))\n",
    "except:\n",
    "    pass\n",
    "print(\"Batches of items: \", (tbatch[\"one_hot_items_values\"]))\n",
    "print(\"Batches of items' weights: \", (tbatch[\"one_hot_items_weights\"]))\n",
    "#print(\"Batches of profiles: \", len(tbatch[\"profiles\"]))\n",
    "print(\"Batches of labels: \", (tbatch[\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [],
   "source": [
    "#batch, labels, data_index = generate_batch(data, 0)\n",
    "#train_labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "#train_batch = tf.convert_to_tensor(batch, dtype=tf.int32)\n",
    "\n",
    "#embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "#embedded_input = tf.nn.embedding_lookup(embeddings, train_batch)\n",
    "#embedded_labels = tf.nn.embedding_lookup(embeddings, train_labels)\n",
    "\n",
    "#distances_matrix = embedded_labels @ tf.transpose(embeddings)\n",
    "#distances_matrix = tf.matmul(embedded_labels, tf.transpose(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # TEST tf.nn.embedding_lookup_sparse #\n",
    "# # https://stackoverflow.com/questions/39207587/how-to-use-tf-nn-embedding-lookup-sparse-in-tensorflow\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/SparseTensor\n",
    "# ######################################\n",
    "\n",
    "# batch_size = 3\n",
    "# max_words = 3\n",
    "# max_items = 2\n",
    "# w_indices = [[i, j] for i in range(0, batch_size) for j in range(0, max_words)]\n",
    "# i_indices = [[i, j] for i in range(0, batch_size) for j in range(0, max_items)]\n",
    "# # indices = [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]  \n",
    "# w_values = [0, 1, 2, 3, 2, 3, 2, 1, 3]   # nel primo input parole 0, 1 e 2; nel sec e terzo parole 3,2 e 3\n",
    "# w_weights = [0, 1, 2, 3, 2, 3, 2, 1, 3]\n",
    "# i_values = [0, 1, 2, 3, 2, 3]   # nel primo input item 0 & 1; nel sec e terzo parole 2 & 3\n",
    "# i_weights = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# w_sp_ids = tf.SparseTensor(indices=w_indices,\n",
    "#                           values=w_values,\n",
    "#                           dense_shape=[batch_size, max_words])\n",
    "\n",
    "# w_sp_weights = tf.SparseTensor(indices=w_indices,\n",
    "#                             values=w_weights,\n",
    "#                             dense_shape=[batch_size, max_words])\n",
    "\n",
    "# i_sp_ids = tf.SparseTensor(indices=i_indices,\n",
    "#                           values=i_values,\n",
    "#                           dense_shape=[batch_size, max_items])\n",
    "\n",
    "# i_sp_weights = tf.SparseTensor(indices=i_indices,\n",
    "#                             values=i_weights,\n",
    "#                             dense_shape=[batch_size, max_items])\n",
    "\n",
    "# vocabulary_size = 10\n",
    "# max_item_id = 5\n",
    "# w_embedding_size = 2\n",
    "# i_embedding_size = 1\n",
    "\n",
    "# w_embeddings = tf.Variable(np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0], [9.0, 10.0], \n",
    "#                 [11.0, 12.0], [13.0, 14.0], [15.0, 16.0], [17.0, 18.0], [19.0, 20.0]]))\n",
    "\n",
    "# i_embeddings = tf.Variable(np.array([[1.0], [4.0], [6.0], [8.0], [10.0], [5.0]]))\n",
    "\n",
    "# w_embed = tf.nn.embedding_lookup_sparse(params=w_embeddings, sp_ids=w_sp_ids, sp_weights=w_sp_weights, combiner='mean')\n",
    "# i_embed = tf.nn.embedding_lookup_sparse(params=i_embeddings, sp_ids=i_sp_ids, sp_weights=i_sp_weights, combiner='mean')\n",
    "\n",
    "# profiles = tf.Variable(np.array([[1.0, -0.9998036896131239, 1.0], [1.0, -0.9998036896131239, 1.0], [-1.0, -0.2260795454368001, 1.0]]))\n",
    "\n",
    "# somma = tf.concat([w_embed, i_embed, profiles], 1)\n",
    "\n",
    "# words_train_input = tf.SparseTensor(indices=words_indices, \n",
    "#                                             values=batch[\"one_hot_words_values\"], \n",
    "#                                             dense_shape=[batch_size, max_words])\n",
    "\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     print(\"w_embed: (of shape %s)\" % session.run(tf.shape(w_embed)))\n",
    "#     print(session.run(w_embed))\n",
    "#     print(\"i_embed: (of shape %s)\" % session.run(tf.shape(i_embed)))\n",
    "#     print(session.run(i_embed))\n",
    "#     print(\"Somma: (of shape %s)\" % session.run(tf.shape(somma)))\n",
    "#     print(session.run(somma))\n",
    "#     print(session.run(tf.sparse_reduce_sum(words_train_input)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embedding_size = 0\n",
    "items_embedding_size = 10\n",
    "n_profile_features = 0\n",
    "relu_dimension = items_embedding_size + int((n_profile_features + words_embedding_size)/2)\n",
    "num_sampled = 32  # number of classes to randomly sample per batch\n",
    "if synthetic:\n",
    "    max_words = 0\n",
    "else:\n",
    "    max_words = 5    \n",
    "max_items = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "from collections import Counter\n",
    "\n",
    "if synthetic:\n",
    "    valid_size = 10\n",
    "    valid_window = 1000\n",
    "    valid_examples = list(range(10))    \n",
    "else:\n",
    "    valid_size = 1 # Random set of items to evaluate similarity on.\n",
    "    valid_window = 1000 # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = [118]  #np.array(random.sample(range(valid_window), valid_size))\n",
    "    user_test = '104415'  # marioalemi\n",
    "    user_test_books = [int(o[1]) for o in clean_obs['104415']]\n",
    "    user_test_book_weights = [1.0 for o in clean_obs['104415']]\n",
    "\n",
    "    my_words = []\n",
    "    for b in user_test_books:\n",
    "        my_words += item_titles[str(b)]\n",
    "\n",
    "    user_test_words = [x[0] for x in Counter(my_words).most_common()[:5]]\n",
    "    user_test_word_weights = [1.] * len(user_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not synthetic:\n",
    "    from itertools import chain\n",
    "    words = set(int(w) for w in chain.from_iterable([title for title in item_titles.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not synthetic:\n",
    "    max(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "max_item_id = max([int(i) for i in items.keys()]) + 1  # different from item_size (some item_id are never used, so the biggest ID is bigger than len(items))\n",
    "try:\n",
    "    vocabulary_size = max(words)\n",
    "except ValueError:\n",
    "    vocabulary_size = 0\n",
    "words_indices = np.array([[i, j] for i in range(0, batch_size) for j in range(0, max_words)], dtype=np.int64)\n",
    "items_indices = np.array([[i, j] for i in range(0, batch_size) for j in range(0, max_items)], dtype=np.int64)\n",
    "\n",
    "print(\"max item id\", max_item_id)\n",
    "print(\"number of features per user's profile\", n_profile_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfr https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(), tf.device(\"/cpu:0\"):\n",
    "\n",
    "    # Input data.\n",
    "    # input_dimensions = max_words + max_items + n_profile_features\n",
    "    # going to be the input of embedding_lookup_sparse for words and items\n",
    "    words_train_input = tf.sparse_placeholder(tf.int64, shape=[batch_size, max_words])\n",
    "    words_weights_train_input = tf.sparse_placeholder(tf.float32, shape=[batch_size, max_words])\n",
    "    \n",
    "    items_train_input = tf.sparse_placeholder(tf.int64, shape=[batch_size, max_items])\n",
    "    items_weights_train_input = tf.sparse_placeholder(tf.float32, shape=[batch_size, max_items])\n",
    "    \n",
    "    if n_profile_features > 0:\n",
    "        profile_train_input = tf.placeholder(tf.float32, shape=[batch_size, n_profile_features])\n",
    "    \n",
    "    train_labels = tf.placeholder(tf.int64, shape=(batch_size, 1))\n",
    "    \n",
    "    # test book similarity\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int64)\n",
    "    \n",
    "    # test recommendations\n",
    "    if not synthetic:\n",
    "        recommendation_books = tf.constant(len(user_test_books), dtype=tf.int64)\n",
    "        recommendation_book_weights = tf.constant(len(user_test_book_weights), dtype=tf.float32)\n",
    "\n",
    "    if words_embedding_size > 0:\n",
    "        recommendation_words = tf.constant(len(user_test_words), dtype=tf.int64)\n",
    "        recommendation_word_weights = tf.constant(len(user_test_word_weights), dtype=tf.float32)\n",
    "    \n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation    \n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        ## Random values to the embedding vectors        \n",
    "        items_embeddings = tf.Variable(tf.random_uniform([max_item_id, items_embedding_size], -1.0, 1.0))\n",
    "        embedded_items = tf.nn.embedding_lookup_sparse(params=items_embeddings,\n",
    "                                                       sp_ids=items_train_input,\n",
    "                                                       sp_weights=items_weights_train_input,\n",
    "                                                       combiner='mean')\n",
    "        \n",
    "        if words_embedding_size > 0:\n",
    "            words_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, words_embedding_size], -1.0, 1.0))\n",
    "\n",
    "            embedded_words = tf.nn.embedding_lookup_sparse(params=words_embeddings,\n",
    "                                                           sp_ids=words_train_input,\n",
    "                                                           sp_weights=words_weights_train_input,\n",
    "                                                           combiner='mean')\n",
    "\n",
    "\n",
    "        # adesso ho i vettori delle parole e degli item (mediati nel tempo), piu` i profili (che non vengono embedded). \n",
    "        # Devo concatenarli tutti e tre per poi creare weights & biases.\n",
    "        #TODO... meglio\n",
    "        if n_profile_features > 0 and words_embedding_size > 0:\n",
    "            total_embedded = tf.concat([embedded_items, embedded_words, profile_train_input], 1)\n",
    "        elif words_embedding_size > 0:\n",
    "            total_embedded = tf.concat([embedded_items, embedded_words], 1)\n",
    "        else:\n",
    "            total_embedded = embedded_items\n",
    "            \n",
    "        # Construct the variables for the NCE loss.\n",
    "        # The concatenated vectors of words, items and profile go into the network and\n",
    "        # the output is a vector which should be the one_hot of the label (trained against then...)\n",
    "\n",
    "        total_embedded_input_size = items_embedding_size + words_embedding_size + n_profile_features\n",
    "\n",
    "        # Go through layer\n",
    "        relu_1_weights = tf.Variable(tf.random_normal([total_embedded_input_size,\n",
    "                                                       items_embedding_size]))\n",
    "        \n",
    "        relu_1_bias = tf.Variable(tf.random_normal([items_embedding_size]))\n",
    "        \n",
    "        relu_1_output = tf.nn.relu_layer(total_embedded, relu_1_weights, relu_1_bias, name=\"relu_1\")\n",
    "            \n",
    "        # TODO\n",
    "        # Brutto. Qui da una concatenazione di embedded tenta di ottenere direttamente il\n",
    "        # one-hot, con conseguente proliferazione di parametri. Sarebbe meglio convogliare il vettore\n",
    "        # di ingresso in un vettore items_embedding_size, e trainarlo con la label embedded.\n",
    "        \"\"\" RILEGGI simulator.py -> recommender_information doc.\n",
    "        La probabilita` di avere label dipende dalla distanza dell'item di input, ma anche da quanto label\n",
    "        vende... tipo voglio minimizzare (distance(0-1) - absolute_probability) o roba simile.\n",
    "        \"\"\"\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([max_item_id, relu_dimension],\n",
    "                                  stddev=1.0 / math.sqrt(relu_dimension)))\n",
    "        nce_biases = tf.Variable(tf.zeros([max_item_id]))\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=relu_1_output,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=max_item_id,\n",
    "                       num_true=1,\n",
    "                       remove_accidental_hits=True  # in case takes a label as negative\n",
    "                      )\n",
    "    )\n",
    "\n",
    "    # The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "#    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # for testing...\n",
    "    y = train_labels\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings\n",
    "    # with the cosine distance (taken from original word2vec example)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(items_embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = items_embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 23
      },
      {
       "item_id": 48
      },
      {
       "item_id": 61
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 436189,
     "status": "ok",
     "timestamp": 1445965429787,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "1bQFGceBxrWW",
    "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# last number is ~hours (100 steps per minutes)\n",
    "num_steps = 100*60*10\n",
    "num_steps = 10000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print('Initialized')    \n",
    "    average_loss = 0.0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "#        try:\n",
    "        batch, data_index = generate_batch_constant(dataset, \n",
    "                                                    item_titles, \n",
    "                                                    data_index=data_index, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    max_words=max_words, \n",
    "                                                    max_items=max_items)        \n",
    "        #print(\"Generated batch for step \", step)\n",
    "\n",
    "# cfr https://www.tensorflow.org/api_docs/python/tf/sparse_placeholder\n",
    "        if words_embedding_size > 0:\n",
    "            sp_words_train_input = tf.SparseTensor(indices=words_indices, \n",
    "                                                values=batch[\"one_hot_words_values\"], \n",
    "                                                dense_shape=[batch_size, max_words])\n",
    "            vsp_words_train_input = sp_words_train_input.eval(session=session)\n",
    "\n",
    "            sp_words_weights_train_input = tf.SparseTensor(indices=words_indices, \n",
    "                                                values=batch[\"one_hot_words_weights\"], \n",
    "                                                dense_shape=[batch_size, max_words])        \n",
    "            vsp_words_weights_train_input = sp_words_weights_train_input.eval(session=session)\n",
    "\n",
    "        sp_items_train_input = tf.SparseTensor(indices=items_indices, \n",
    "                                            values=batch[\"one_hot_items_values\"], \n",
    "                                            dense_shape=[batch_size, max_item_id])\n",
    "        vsp_items_train_input = sp_items_train_input.eval(session=session)\n",
    "\n",
    "        sp_items_weights_train_input = tf.SparseTensor(indices=items_indices, \n",
    "                                            values=batch[\"one_hot_items_weights\"], \n",
    "                                            dense_shape=[batch_size, max_item_id])\n",
    "        vsp_items_weights_train_input = sp_items_weights_train_input.eval(session=session)\n",
    "\n",
    "        if words_embedding_size > 0:\n",
    "            feed_dict={\n",
    "                words_train_input: vsp_words_train_input,\n",
    "                words_weights_train_input: vsp_words_weights_train_input,\n",
    "                items_train_input: vsp_items_train_input,\n",
    "                items_weights_train_input: vsp_items_weights_train_input,\n",
    "    #            profile_train_input: batch[\"profiles\"],\n",
    "                train_labels: batch[\"labels\"]\n",
    "                }\n",
    "        else:\n",
    "            feed_dict={\n",
    "                items_train_input: vsp_items_train_input,\n",
    "                items_weights_train_input: vsp_items_weights_train_input,\n",
    "    #            profile_train_input: batch[\"profiles\"],\n",
    "                train_labels: batch[\"labels\"]\n",
    "                }\n",
    "            \n",
    "#         print(\"TESTER: \", session.run(y, feed_dict=feed_dict))  # Just test\n",
    "        _, lozz = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        #print(\">>>>>>>>>>>>> \", lozz, step, num_steps)\n",
    "#         if np.isnan(lozz):   \n",
    "#             print(\"TESTER: \", session.run(y, feed_dict=feed_dict))  # Just test\n",
    "#             print(\"step \", step)\n",
    "#             continue\n",
    "        average_loss += lozz\n",
    "#         except:\n",
    "#             print(\"Step %s no buono\" % step)\n",
    "        if step % 500 == 0:\n",
    "            print(\"=========================================\\n\\nDone step \", step)\n",
    "            average_loss = average_loss / 500\n",
    "            print(\"Average loss:\", average_loss)    \n",
    "            average_loss = 0.0\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                if synthetic:\n",
    "                    valid_item = valid_examples[i]\n",
    "                else:\n",
    "                    valid_item = bookid_title[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_item\n",
    "                for k in range(top_k):\n",
    "                    if synthetic:\n",
    "                        close_item = nearest[k]\n",
    "                    else:\n",
    "                        close_item = bookid_title[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_item)\n",
    "                print(log)\n",
    "            \n",
    "#            embeds = embeddings.eval()\n",
    "                \n",
    "#                print(\"Distanza: \", distanza(embeds[1395], embeds[810]))            \n",
    "            \n",
    "      \n",
    "#    distance_embeddings = embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dataset_list[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid = title_bookid['lorenzo carrara_favole celtiche']\n",
    "for o in test_obs_dataset_list:\n",
    "    if lid in [t[1] for t in o[2]]:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of Deep Learning\n",
    "[https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/blob/master/archive/skipgram.py]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(x, embedding_shape):\n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        embedding_init = tf.random_uniform(embedding_shape, -1.0, 1.0)\n",
    "        embedding_matrix = tf.get_variable(\"E\", initializer=embedding_init)\n",
    "        return tf.nn.embedding_lookup(embedding_matrix, x), embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_contrastive_loss(embedding_lookup, weight_shape, bias_shape, y):\n",
    "    with tf.variable_scope(\"nce\"):\n",
    "        nce_weight_init = tf.truncated_normal(weight_shape, stddev=1.0/(weight_shape[1])**0.5)\n",
    "        nce_bias_init = tf.zeros(bias_shape)\n",
    "        nce_W = tf.get_variable(\"W\", initializer=nce_weight_init)\n",
    "        nce_b = tf.get_variable(\"b\", initializer=nce_bias_init)\n",
    "\n",
    "        total_loss = tf.nn.nce_loss(nce_W, nce_b, embedding_lookup, y, neg_size, data.vocabulary_size)\n",
    "        return tf.reduce_mean(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        summary_op = tf.scalar_summary(\"cost\", cost)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "        return train_op, summary_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jjJXYA_XzV79"
   },
   "outputs": [],
   "source": [
    "def validation(embedding_matrix, x_val):\n",
    "    norm = tf.reduce_sum(embedding_matrix**2, 1, keep_dims=True)**0.5\n",
    "    normalized = embedding_matrix/norm\n",
    "    val_embeddings = tf.nn.embedding_lookup(normalized, x_val)\n",
    "    cosine_similarity = tf.matmul(val_embeddings, normalized, transpose_b=True)\n",
    "    return normalized, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1445965465525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "o_e0D_UezcDe",
    "outputId": "df22e4a5-e8ec-4e5e-d384-c6cf37c68c34"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB5EFrBnpNnc"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
